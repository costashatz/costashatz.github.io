<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport"    content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author"      content="Sergey Pozhilov (GetTemplate.com)">

  <title>Konstantinos Chatzilygeroudis | Videos</title>

  <link rel="shortcut icon" href="assets/images/favicon.gif">

  <!-- Bootstrap -->
  <link href="http://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.no-icons.min.css" rel="stylesheet">
  <!-- Icons -->
  <link href="http://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
  <!-- Fonts -->
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=Alice|Open+Sans:400,300,700">
  <!-- Custom styles -->
  <link rel="stylesheet" href="assets/css/styles.css">
  <link rel="stylesheet" href="assets/css/academicons.css">

  <!--[if lt IE 9]> <script src="assets/js/html5shiv.js"></script> <![endif]-->
</head>
<body>

<header id="header">
  <div id="head" class="parallax" parallax-speed="2">
    <h1 id="logo" class="text-center">
      <img class="img-circle" src="assets/images/profile.jpg" alt="">
      <span class="title">Konstantinos Chatzilygeroudis</span>
      <span class="tagline">PhD Candidate in Robotics and Machine Learning<br>
        <a href="http://www.inria.fr/en/centre/nancy" target="_blank">Inria Nancy</a> - <a href="https://team.inria.fr/larsen/" target="_blank">LARSEN Team</a><br>
        <a href="mailto:costashatz@gmail.com">costashatz@gmail.com</a></span>
    </h1>
  </div>

  <nav class="navbar navbar-default navbar-sticky">
    <div class="container-fluid">

      <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button>
      </div>

      <div class="navbar-collapse collapse">

        <ul class="nav navbar-nav">
          <li class="active"><a href="./index.html">Home</a></li>
          <!-- <li><a href="./research.html">Research</a></li> -->
          <li><a href="./publications.html">Publications</a></li>
          <li><a href="./files/Konstantinos_Chatzilygeroudis_CV.pdf" target="_blank">CV</a></li>
          <li><a href="./videos.html">Videos</a></li>
          <li><a href="./contact.html">Contact</a></li>
        </ul>

      </div><!--/.nav-collapse -->
    </div>
  </nav>
</header>

<main id="main">
  <div class="container">

    <div class="row topspace">
      <h2 class="section-title">
        <span>ICRA 2018</span>
      </h2>
      <div class="col-sm-8 col-sm-offset-2">

        <article class="post">
          <header class="entry-header">
            <div class="entry-meta">
              <span class="posted-on">
                <time class="entry-date published" date="2018-01-12">January 12, 2018</time>
              </span>
            </div>
            <h1 class="entry-content">
              <center>Using Parameterized Black-Box Priors to Scale Up Model-Based Policy Search for Robotics</center>
            </h1>
          </header>
          <div class="entry-content">
            <p>
              <center>
                <iframe width="560" height="315" src="https://www.youtube.com/embed/HFkZkhGGzTo" frameborder="0" allowfullscreen></iframe>
              </center>
            </p>
            <p>
              <b>"Using Parameterized Black-Box Priors to Scale Up Model-Based Policy Search for Robotics"</b><br/>
              <i>Konstantinos Chatzilygeroudis</i> and <i>Jean-Baptiste Mouret</i>
              <br/>Paper to be presented at the <b>International Conference on Robotics and Automation</b> (ICRA) 2018<br/><br/>
            </p>
            <p>
              <b>Abstract:</b>
              </br/>
              <br/> The most data-efficient algorithms for reinforcement learning in robotics are model-based policy search algorithms, which alternate between learning a dynamical model of the robot and optimizing a policy to maximize the expected return given the model and its uncertainties. Among the few proposed approaches, the recently introduced Black-DROPS algorithm exploits a black-box optimization algorithm to achieve both high data-efficiency and good computation times when several cores are used; nevertheless, like all model-based policy search approaches, Black-DROPS does not scale to high dimensional state/action spaces. In this paper, we introduce a new model learning procedure in Black-DROPS that leverages parameterized black-box priors to (1) scale up to high-dimensional systems, and (2) be robust to large inaccuracies of the prior information. We demonstrate the effectiveness of our approach with the "pendubot" swing-up task in simulation and with a physical hexapod robot (48D state space, 18D action space) that has to walk forward as fast as possible. The results show that our new algorithm is more data-efficient than previous model-based policy search algorithms (with and without priors) and that it can allow a physical 6-legged robot to learn new gaits in only 16 to 30 seconds of interaction time.
              <br/><br/>

              This work was supported by the ERC project <a href="http://www.resibots.eu">“ResiBots”</a> (grant agreement No 637972), funded by the European Research Council.
            </p>
          </div>
        </article>
        <!-- #post-## -->
      </div>

      <div class="col-sm-8 col-sm-offset-2">
        <article class="post">
          <header class="entry-header">
            <div class="entry-meta">
              <span class="posted-on">
                <time class="entry-date published" date="2018-01-12">January 12, 2018</time>
              </span>
            </div>
            <h1 class="entry-content">
              <center>Bayesian Optimization with Automatic Prior Selection for Data-Efficient Direct Policy Search</center>
            </h1>
          </header>
          <div class="entry-content">
            <p>
              <center>
                <iframe width="560" height="315" src="https://www.youtube.com/embed/xo8mUIZTvNE" frameborder="0" allowfullscreen></iframe>
              </center>
            </p>
            <p>
              <b>"Bayesian Optimization with Automatic Prior Selection for Data-Efficient Direct Policy Search"</b><br/>
              <i>Rémi Pautrat, Konstantinos Chatzilygeroudis</i> and <i>Jean-Baptiste Mouret</i>
              <br/>Paper to be presented at the <b>International Conference on Robotics and Automation</b> (ICRA) 2018. <i>A short version of the paper was accepted at the non-archival track of the 1st Conference on Robot Learning (CoRL) 2017.</i><br/><br/>
            </p>
            <p>
              <b>Abstract:</b>
              </br/>
              <br/> One of the most interesting features of Bayesian optimization for direct policy search is that it can leverage priors (e.g., from simulation or from previous tasks) to accelerate learning on a robot. In this paper, we are interested in situations for which several priors exist but we do not know in advance which one fits best the current situation. We tackle this problem by introducing a novel acquisition function, called Most Likely Expected Improvement (MLEI), that combines the likelihood of the priors and the expected improvement. We evaluate this new acquisition function on a transfer learning task for a 5-DOF planar arm and on a possibly damaged, 6-legged robot that has to learn to walk on flat ground and on stairs, with priors corresponding to different stairs and different kinds of damages. Our results show that MLEI effectively identifies and exploits the priors, even when there is no obvious match between the current situations and the priors.
              <br/><br/>

              This work was supported by the ERC project <a href="http://www.resibots.eu">“ResiBots”</a> (grant agreement No 637972), funded by the European Research Council.
            </p>
          </div>
        </article>
        <!-- #post-## -->
      </div>
    </div>

    <div class="row topspace">
        <h2 class="section-title">
          <span>RAS 2018</span>
        </h2>
        <div class="col-sm-8 col-sm-offset-2">

          <article class="post">
            <header class="entry-header">
              <div class="entry-meta">
                <span class="posted-on">
                  <time class="entry-date published" date="2017-11-22">November 22, 2017</time>
                </span>
              </div>
              <h1 class="entry-content">
                <center>Reset-free Trial-and-Error Learning for Robot Damage Recovery</center>
              </h1>
            </header>
            <div class="entry-content">
              <p>
                <center>
                  <iframe width="560" height="315" src="https://www.youtube.com/embed/IqtyHFrb3BU" frameborder="0" allowfullscreen></iframe>
                </center>
              </p>
              <p>
                <b>"Reset-free Trial-and-Error Learning for Robot Damage Recovery"</b><br/>
                <i>Konstantinos Chatzilygeroudis, Vassilis Vassiliades</i> and <i>Jean-Baptiste Mouret</i>
                <br/><b>Robotics and Autonomous Systems</b> (RAS) 2018<br/><br/>
              </p>
              <p>
                <b>Abstract:</b>
                </br/>
                <br/> The high probability of hardware failures prevents many advanced robots (e.g., legged robots) from being confidently deployed in real-world situations (e.g., post-disaster rescue). Instead of attempting to diagnose the failures, robots could adapt by trial-and-error in order to be able to complete their tasks. In this situation, damage recovery can be seen as a Reinforcement Learning (RL) problem. However, the best RL algorithms for robotics require the robot and the environment to be reset to an initial state after each episode, that is, the robot is not learning autonomously. In addition, most of the RL methods for robotics do not scale well with complex robots (e.g., walking robots) and either cannot be used at all or take too long to converge to a solution (e.g., hours of learning). In this paper, we introduce a novel learning algorithm called "Reset-free Trial-and-Error" (RTE) that (1) breaks the complexity by pre-generating hundreds of possible behaviors with a dynamics simulator of the intact robot, and (2) allows complex robots to quickly recover from damage while completing their tasks and taking the environment into account. We evaluate our algorithm on a simulated wheeled robot, a simulated six-legged robot, and a real six-legged walking robot that are damaged in several ways (e.g., a missing leg, a shortened leg, faulty motor, etc.) and whose objective is to reach a sequence of targets in an arena. Our experiments show that the robots can recover most of their locomotion abilities in an environment with obstacles, and without any human intervention.
                <br/><br/>

                This work was supported by the ERC project <a href="http://www.resibots.eu">“ResiBots”</a> (grant agreement No 637972), funded by the European Research Council.
              </p>
            </div>
          </article>
          <!-- #post-## -->
        </div>
      </div>

    <div class="row topspace">
      <h2 class="section-title"><span>IROS 2017</span></h2>
      <div class="col-sm-8 col-sm-offset-2">

        <article class="post">
          <header class="entry-header">
             <div class="entry-meta">
               <span class="posted-on"><time class="entry-date published" date="2017-07-25">July 25, 2017</time></span>
             </div>
             <h1 class="entry-content"><center>Black-Box Data-efficient Policy Search for Robotics</center></h1>
          </header>
          <div class="entry-content">
            <p><center><iframe width="560" height="315" src="https://www.youtube.com/embed/kTEyYiIFGPM" frameborder="0" allowfullscreen></iframe></center></p>
            <p>
            <b>"Black-Box Data-efficient Policy Search for Robotics"</b><br/>
            <i>Konstantinos Chatzilygeroudis, Roberto Rama, Rituraj Kaushik, Dorian Goepp, Vassilis Vassiliades</i> and <i>Jean-Baptiste Mouret</i><br/>
            Paper presented at International Conference on Intelligent Robots and Systems <b>(IROS)</b> 2017.<br/><br/></p>
            <p><b>Abstract:</b></br/><br/>
              The most data-efficient algorithms for reinforcement learning (RL) in robotics are based on uncertain dynamical models: after each episode, they first learn a dynamical model of the robot, then they use an optimization algorithm to find a policy that maximizes the expected return given the model and its uncertainties. It is often believed that this optimization can be tractable only if analytical, gradient-based algorithms are used; however, these algorithms require using specific families of reward functions and policies, which greatly limits the flexibility of the overall approach. In this paper, we introduce a novel model-based RL algorithm, called Black-DROPS (Black-box Data-efficient RObot Policy Search) that: (1) does not impose any constraint on the reward function or the policy (they are treated as black-boxes), (2) is as data-efficient as the state-of-the-art algorithm for data-efficient RL in robotics, and (3) is as fast (or faster) than analytical approaches when several cores are available. The key idea is to replace the gradient-based optimization algorithm with a parallel, black-box algorithm that takes into account the model uncertainties. We demonstrate the performance of our new algorithm on two standard control benchmark problems (in simulation) and a low-cost robotic manipulator (with a real robot).<br/><br/>

              This work was supported by the ERC project <a href="http://www.resibots.eu">“ResiBots”</a> (grant agreement No 637972), funded by the European Research Council.
            </p>
          </div>
        </article><!-- #post-## -->
      </div>
    </div>

    <div class="row topspace">
      <h2 class="section-title"><span>NIPS 2016 Bayesian Optimization Workshop</span></h2>
      <div class="col-sm-8 col-sm-offset-2">

        <article class="post">
          <header class="entry-header">
             <div class="entry-meta">
               <span class="posted-on"><time class="entry-date published" date="2016-04-05">Dec 4, 2016</time></span>
             </div>
             <h1 class="entry-content"><center>Safety-Aware Robot Damage<br/>Recovery Using Constrained Bayesian Optimization and Simulated Priors</center></h1>
          </header>
          <div class="entry-content">
            <p><center><iframe width="560" height="315" src="https://www.youtube.com/embed/8esrj-7WhsQ" frameborder="0" allowfullscreen></iframe></center></p>
            <p>
            <b>"Safety-Aware Robot Damage Recovery Using Constrained Bayesian Optimization and Simulated Priors"</b><br/>
            <i>Vaios Papaspyros, Konstantinos Chatzilygeroudis, Vassilis Vassiliades</i> and <i>Jean-Baptiste Mouret</i><br/>
            Paper presented at "Bayesian Optimization: Black-box Optimization and Beyond" (BayesOpt) workshop in NIPS 2016.<br/><br/></p>
            <p><b>Abstract:</b></br/><br/>
              The recently introduced Intelligent Trial-and-Error (IT&E) algorithm showed that robots can adapt to damage in a matter of a few trials. The success of this algorithm relies on two components: prior knowledge acquired through simulation with an intact robot, and Bayesian optimization (BO) that operates on-line, on the damaged robot. While IT&E leads to fast damage recovery, it does not incorporate any safety constraints that prevent the robot from attempting harmful behaviors. In this work, we address this limitation by replacing the BO component with a constrained BO procedure. We evaluate our approach on a simulated damaged humanoid robot that needs to crawl as fast as possible, while performing as few unsafe trials as possible. We compare our new "safety-aware IT&E" algorithm to IT&E and a multi-objective version of IT&E in which the safety constraints are dealt as separate objectives. Our results show that our algorithm outperforms the other approaches, both in crawling speed within the safe regions and number of unsafe trials.<br/><br/>
              This video shows a damaged simulated iCub robot safely compensating for damage.<br/><br/>
              This work was supported by the ERC project <a href="http://www.resibots.eu">“ResiBots”</a> (grant agreement No 637972), funded by the European Research Council.
            </p>
            <p><b>Poster </b>(click on the image for the pdf)<b>:</b></br></br>
              <a href="./files/nips-poster-2016.pdf"><img src="./files/nips-poster-2016.png"/></a>
            </p>
          </div>
        </article><!-- #post-## -->
      </div>
    </div>

    <div class="row topspace">
      <h2 class="section-title"><span>ICRA 2016 AILTA Workshop</span></h2>
      <div class="col-sm-8 col-sm-offset-2">

        <article class="post">
          <header class="entry-header">
             <div class="entry-meta">
               <span class="posted-on"><time class="entry-date published" date="2016-04-05">April 5, 2016</time></span>
             </div>
             <h1 class="entry-content"><center>Towards semi-episodic learning for robot damage recovery</center></h1>
          </header>
          <div class="entry-content">
            <p><center><iframe width="560" height="315" src="https://www.youtube.com/embed/Gpf5h07pJFA" frameborder="0" allowfullscreen></iframe></center></p>
            <p>
            <b>"Towards semi-episodic learning for robot damage recovery"</b><br/>
            <i>Konstantinos Chatzilygeroudis, Antoine Cully</i> and <i>Jean-Baptiste Mouret</i><br/>
            Paper presented (20 min talk) at "Artificial Intelligence for Long-Term Autonomy" (AILTA) workshop in ICRA 2016.<br/><br/></p>
            <p><b>Abstract:</b></br/><br/>
              The recently introduced Intelligent Trial and Error algorithm (IT&E) enables robots to creatively adapt to damage in a matter of minutes by combining an off-line evolutionary algorithm and an on-line learning algorithm based on Bayesian Optimization. We extend the IT&E algorithm to allow for robots to learn to compensate for damages while executing their task(s). This leads to a semi-episodic learning scheme that increases the robot’s life-time autonomy and adaptivity. Preliminary experiments on a toy simulation and a 6-legged robot locomotion task show promising results.<br/><br/>
              This video shows a 6-legged robot performing locomotion tasks despite the left middle leg being removed using our technique.<br/><br/>
              This work was supported by the ERC project <a href="http://www.resibots.eu">“ResiBots”</a> (grant agreement No 637972), funded by the European Research Council.
            </p>
            <p><b>Poster </b>(click on the image for the pdf)<b>:</b></br></br>
              <a href="./files/sela-poster-ailta-2016.pdf"><img src="./files/sela-poster-ailta-2016.png"/></a>
            </p>
          </div>
        </article><!-- #post-## -->

      </div>
      <h2 class="section-title"><span>Diploma Thesis</span></h2>
      <div class="col-sm-8 col-sm-offset-2">

        <article class="post">
          <header class="entry-header">
             <div class="entry-meta">
               <span class="posted-on"><time class="entry-date published" date="2015-09-15">September 15, 2015</time></span>
             </div>
             <h1 class="entry-content"><center>NAO Walking Simulation in Gazebo</center></h1>
          </header>
          <div class="entry-content">
            <p><center><iframe width="560" height="315" src="https://www.youtube.com/embed/xPg7caI26Z4" frameborder="0" allowfullscreen></iframe></center></p>
            <p>
              Part of my diploma thesis. NAO Humanoid Walking Simulation in Gazebo using NAOqi Simulator C++ SDK. More information on <a href="https://github.com/costashatz/nao_gazebo" target="_blank">github repo</a>.
            </p>
          </div>
        </article><!-- #post-## -->

      </div>

    </div> <!-- /section -->

  </div>  <!-- /container -->

</main>

<footer id="footer">
  <div class="container">
    <div class="row">
      <div class="col-md-4 widget">
        <h3 class="widget-title">Contact</h3>
        <div class="widget-body">
          <p>+33 610466287<br>
            <a href="mailto:konstantinos.chatzilygeroudis@inria.fr">konstantinos.chatzilygeroudis@inria.fr</a><br>
            <a href="mailto:costashatz@gmail.com">costashatz@gmail.com</a><br>
            <br>
            <a href="http://www.inria.fr/en/centre/nancy" target="_blank">Inria Nancy - Grand Est</a><br>
            <a href="https://team.inria.fr/larsen/" target="_blank">LARSEN Team</a><br>
            Room C127<br>
            615 rue du Jardin Botanique,<br>
            54600 Villers-lès-Nancy, France
          </p>
        </div>
      </div>

      <div class="col-md-4 widget">
        <h3 class="widget-title">Research</h3>
        <div class="widget-body">
          <p class="follow-me-icons">
            <a href="https://scholar.google.gr/citations?user=tnf6B-EAAAAJ&hl=en" class="ai ai-google-scholar" style="text-decoration: none;" target="_blank"></a>
            <a href="https://www.researchgate.net/profile/Konstantinos_Chatzilygeroudis" class="ai ai-researchgate" style="text-decoration: none;" target="_blank"></a>
            <a href="https://www.zotero.org/costashatz" class="ai ai-zotero" style="text-decoration: none;" target="_blank"></a>
          </p>
        </div>
      </div>

      <div class="col-md-4 widget">
        <h3 class="widget-title">Follow me</h3>
        <div class="widget-body">
          <p class="follow-me-icons">
            <a href="https://github.com/costashatz" target="_blank"><i class="fa fa-github fa-2"></i></a>
            <a href="https://bitbucket.org/costashatz" target="_blank"><i class="fa fa-bitbucket fa-2"></i></a>
            <a href="https://gr.linkedin.com/in/konstantinoschatzilygeroudis" target="_blank"><i class="fa fa-linkedin fa-2"></i></a>
            <a href="https://www.facebook.com/CostasHatz" target="_blank"><i class="fa fa-facebook fa-2"></i></a>
            <a href="https://plus.google.com/+KonstantinosChatzilygeroudis" target="_blank"><i class="fa fa-google-plus fa-2"></i></a>
            <a href="https://twitter.com/chatzilyge" target="_blank"><i class="fa fa-twitter fa-2"></i></a>
          </p>
        </div>
      </div>

    </div> <!-- /row of widgets -->
  </div>
</footer>

<footer id="underfooter">
  <div class="container">
    <div class="row">
      <div class="col-md-12 widget">
        <div class="widget-body">
          <p class="text-right">
            Copyright &copy; 2015, Konstantinos Chatzilygeroudis<br>
            Design: <a href="http://www.gettemplate.com" rel="designer">Initio by GetTemplate</a> </p>
        </div>
      </div>

    </div> <!-- /row of widgets -->
  </div>
</footer>



<!-- JavaScript libs are placed at the end of the document so the pages load faster -->
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="http://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>
<script src="assets/js/template.js"></script>
</body>
</html>
